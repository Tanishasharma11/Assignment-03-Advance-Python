{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkOSd+i6HnxrpYlghlkXaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanishasharma11/Assignment-03-Advance-Python/blob/main/Assignment_03_Advanced_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Process Automation**"
      ],
      "metadata": {
        "id": "qyuya9NcP5_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Create a file that contains 1000 lines of random strings."
      ],
      "metadata": {
        "id": "CG3q2N8bP3eO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nae7dVtHPxTg",
        "outputId": "14c95455-35d1-4d0f-9d03-42370dc54cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created successfully.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "# Function to generate a random string\n",
        "def generate_random_string(length):\n",
        "    letters = string.ascii_letters\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Number of lines in the file\n",
        "num_lines = 1000\n",
        "\n",
        "# Length of each random string\n",
        "string_length = 10\n",
        "\n",
        "# Open the file in write mode\n",
        "with open('random_strings.txt', 'w') as file:\n",
        "    # Generate and write random strings to the file\n",
        "    for _ in range(num_lines):\n",
        "        random_string = generate_random_string(string_length)\n",
        "        file.write(random_string + '\\n')\n",
        "\n",
        "print(\"File created successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Create a file that contains multiple lines of random strings and file size must be 5 MB."
      ],
      "metadata": {
        "id": "eeOcpc7rQJIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Function to generate a random string\n",
        "def generate_random_string(length):\n",
        "    letters = string.ascii_letters\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Target file size in bytes (5 MB)\n",
        "target_size = 5 * 1024 * 1024\n",
        "\n",
        "# Length of each random string\n",
        "string_length = 10\n",
        "\n",
        "# Open the file in write mode\n",
        "with open('random_strings.txt', 'w') as file:\n",
        "    file_size = 0\n",
        "\n",
        "    # Generate and write random strings until the target file size is reached\n",
        "    while file_size < target_size:\n",
        "        random_string = generate_random_string(string_length)\n",
        "        file.write(random_string + '\\n')\n",
        "\n",
        "        # Update the file size\n",
        "        file_size = os.path.getsize('random_strings.txt')\n",
        "\n",
        "print(\"File created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaTSqPYIQWYQ",
        "outputId": "dc7a9460-347f-4d4a-9226-ade2c37f1f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Create 10 files that contains multiple lines of random strings and file size of each file must be 5 MB."
      ],
      "metadata": {
        "id": "H4r28fT5QYwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Function to generate a random string\n",
        "def generate_random_string(length):\n",
        "    letters = string.ascii_letters\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# Target file size in bytes (5 MB)\n",
        "target_size = 5 * 1024 * 1024\n",
        "\n",
        "# Number of files to create\n",
        "num_files = 10\n",
        "\n",
        "# Length of each random string\n",
        "string_length = 10\n",
        "\n",
        "# Loop to create multiple files\n",
        "for i in range(num_files):\n",
        "    file_name = f'random_strings_{i}.txt'\n",
        "\n",
        "    # Open the file in write mode\n",
        "    with open(file_name, 'w') as file:\n",
        "        file_size = 0\n",
        "\n",
        "        # Generate and write random strings until the target file size is reached\n",
        "        while file_size < target_size:\n",
        "            random_string = generate_random_string(string_length)\n",
        "            file.write(random_string + '\\n')\n",
        "\n",
        "            # Update the file size\n",
        "            file_size = os.path.getsize(file_name)\n",
        "\n",
        "    print(f\"File {file_name} created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrr6qWfFQdRe",
        "outputId": "77959777-4b38-4fbb-f098-a0a3d4da2c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File random_strings_0.txt created successfully.\n",
            "File random_strings_1.txt created successfully.\n",
            "File random_strings_2.txt created successfully.\n",
            "File random_strings_3.txt created successfully.\n",
            "File random_strings_4.txt created successfully.\n",
            "File random_strings_5.txt created successfully.\n",
            "File random_strings_6.txt created successfully.\n",
            "File random_strings_7.txt created successfully.\n",
            "File random_strings_8.txt created successfully.\n",
            "File random_strings_9.txt created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Create 5 files of size 1GB, 2GB, 3GB, 4GB and 5GB; file contains multiple lines of random strings."
      ],
      "metadata": {
        "id": "SIpBXmgzQmqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Function to generate a random string\n",
        "def generate_random_string(length):\n",
        "    letters = string.ascii_letters\n",
        "    return ''.join(random.choice(letters) for _ in range(length))\n",
        "\n",
        "# File sizes in bytes\n",
        "file_sizes = [1_073_741_824, 2_147_483_648, 3_221_225_472, 4_294_967_296, 5_368_709_120]\n",
        "\n",
        "# Length of each random string\n",
        "string_length = 10\n",
        "\n",
        "# Loop to create files of different sizes\n",
        "for i, file_size in enumerate(file_sizes):\n",
        "    file_name = f'random_strings_{i + 1}GB.txt'\n",
        "\n",
        "    # Open the file in write mode\n",
        "    with open(file_name, 'w') as file:\n",
        "        file_size_generated = 0\n",
        "\n",
        "        # Generate and write random strings until the target file size is reached\n",
        "        while file_size_generated < file_size:\n",
        "            random_string = generate_random_string(string_length)\n",
        "            file.write(random_string + '\\n')\n",
        "\n",
        "            # Update the file size\n",
        "            file_size_generated = os.path.getsize(file_name)\n",
        "\n",
        "    print(f\"File {file_name} created successfully.\")\n"
      ],
      "metadata": {
        "id": "m8BlwSnxQyD-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Convert all the files of Q4 into upper case one by one"
      ],
      "metadata": {
        "id": "sM-9CPBTQ0vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop to convert the files to uppercase\n",
        "for i, file_size in enumerate(file_sizes):\n",
        "    input_file_name = f'random_strings_{i + 1}GB.txt'\n",
        "    output_file_name = f'random_strings_{i + 1}GB_uppercase.txt'\n",
        "\n",
        "    # Open the input file in read mode and the output file in write mode\n",
        "    with open(input_file_name, 'r') as input_file, open(output_file_name, 'w') as output_file:\n",
        "        # Read the input file line by line, convert to uppercase, and write to the output file\n",
        "        for line in input_file:\n",
        "            output_file.write(line.upper())\n",
        "\n",
        "    print(f\"File {output_file_name} created successfully with uppercase content.\")\n"
      ],
      "metadata": {
        "id": "eKjT1L0LRARZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Convert all the files of Q4 into upper case parallel using multi-threading.\n"
      ],
      "metadata": {
        "id": "HlvDTfUcRC1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "\n",
        "# Function to convert a file to uppercase\n",
        "def convert_to_uppercase(input_file_name, output_file_name):\n",
        "    with open(input_file_name, 'r') as input_file, open(output_file_name, 'w') as output_file:\n",
        "        for line in input_file:\n",
        "            output_file.write(line.upper())\n",
        "\n",
        "# Loop to convert the files to uppercase in parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = []\n",
        "\n",
        "    for i, file_size in enumerate(file_sizes):\n",
        "        input_file_name = f'random_strings_{i + 1}GB.txt'\n",
        "        output_file_name = f'random_strings_{i + 1}GB_uppercase.txt'\n",
        "\n",
        "        # Submit the conversion function to the executor and store the future object\n",
        "        future = executor.submit(convert_to_uppercase, input_file_name, output_file_name)\n",
        "        futures.append(future)\n",
        "\n",
        "    # Wait for all the futures to complete\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "print(\"All files converted to uppercase successfully in parallel.\")\n"
      ],
      "metadata": {
        "id": "GUhkIxtWRIww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. WAP to automatically download 10 images of cat from “Google Images”. [Hint: Find the package from \n",
        "pypi.org and use it]\n"
      ],
      "metadata": {
        "id": "ZcQg7F5NRQ91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install google_images_download\n",
        "from google_images_download import google_images_download\n",
        "\n",
        "# Create an instance of the GoogleImagesDownload class\n",
        "response = google_images_download.googleimagesdownload()\n",
        "\n",
        "# Define the search query and other download parameters\n",
        "search_query = \"cat\"\n",
        "num_images = 10\n",
        "download_directory = \"cat_images\"\n",
        "\n",
        "# Specify the arguments for image download\n",
        "arguments = {\n",
        "    \"keywords\": search_query,\n",
        "    \"limit\": num_images,\n",
        "    \"output_directory\": download_directory,\n",
        "    \"no_directory\": True,\n",
        "    \"chromedriver\": \"path_to_chromedriver\"  # Path to chromedriver executable (required for download)\n",
        "}\n",
        "\n",
        "# Download the images\n",
        "response.download(arguments)\n",
        "\n"
      ],
      "metadata": {
        "id": "W0MyESQKRa7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. WAP to automatically download 10 videos of “Machine Learning” from “Youtube.com”. [Hint: Find the \n",
        "package from pypi.org and use it]\n"
      ],
      "metadata": {
        "id": "Q6ptLYOaRyAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install youtube_dl\n",
        "import youtube_dl\n",
        "\n",
        "# Define the search query and other download parameters\n",
        "search_query = \"Machine Learning\"\n",
        "num_videos = 10\n",
        "download_directory = \"./machine_learning_videos/\"\n",
        "\n",
        "# Set the options for video download\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',  # Preferred video format\n",
        "    'outtmpl': download_directory + '%(title)s.%(ext)s',  # Output file name template\n",
        "    'max_downloads': num_videos  # Maximum number of videos to download\n",
        "}\n",
        "\n",
        "# Create the YouTube downloader object\n",
        "ydl = youtube_dl.YoutubeDL(ydl_opts)\n",
        "\n",
        "# Download the videos\n",
        "with ydl:\n",
        "    search_results = ydl.extract_info(\"ytsearch\" + str(num_videos) + \":\" + search_query, download=False)\n",
        "    for result in search_results['entries']:\n",
        "        video_url = result['webpage_url']\n",
        "        ydl.download([video_url])\n"
      ],
      "metadata": {
        "id": "p8_uuttJRqmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Convert all the videos of Q8 and convert it to audio. [Hint: Find the package from pypi.org and use it]\n"
      ],
      "metadata": {
        "id": "t20fMfyER-U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install moviepy\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Define the input and output directories\n",
        "input_directory = \"./machine_learning_videos/\"\n",
        "output_directory = \"./machine_learning_audios/\"\n",
        "\n",
        "# Loop through the downloaded video files\n",
        "for i in range(1, 11):\n",
        "    video_file = f\"{input_directory}Machine Learning-{i}.mp4\"\n",
        "    audio_file = f\"{output_directory}Machine Learning-{i}.mp3\"\n",
        "\n",
        "    # Load the video file\n",
        "    video_clip = VideoFileClip(video_file)\n",
        "\n",
        "    # Extract the audio and save as MP3\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile(audio_file)\n",
        "\n",
        "    # Close the video and audio clips\n",
        "    video_clip.close()\n",
        "    audio_clip.close()\n",
        "\n",
        "    print(f\"Video {i} converted to audio successfully.\")\n",
        "\n",
        "print(\"All videos converted to audio.\")\n"
      ],
      "metadata": {
        "id": "qo8wLLDkSF-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Create an automated pipeline using multi-threading for:\n",
        "“Automatic Download of 100 Videos from YouTube” → “Convert it to Audio”"
      ],
      "metadata": {
        "id": "MOaEjr8ySO27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import youtube_dl\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Define the search query and other download parameters\n",
        "search_query = \"Machine Learning\"\n",
        "num_videos = 100\n",
        "download_directory = \"./machine_learning_videos/\"\n",
        "output_directory = \"./machine_learning_audios/\"\n",
        "\n",
        "# Set the options for video download\n",
        "ydl_opts = {\n",
        "    'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',  # Preferred video format\n",
        "    'outtmpl': download_directory + '%(title)s.%(ext)s',  # Output file name template\n",
        "    'max_downloads': num_videos  # Maximum number of videos to download\n",
        "}\n",
        "\n",
        "# Function to convert a video file to audio\n",
        "def convert_to_audio(video_file, audio_file):\n",
        "    video_clip = VideoFileClip(video_file)\n",
        "    audio_clip = video_clip.audio\n",
        "    audio_clip.write_audiofile(audio_file)\n",
        "    video_clip.close()\n",
        "    audio_clip.close()\n",
        "\n",
        "# Create the YouTube downloader object\n",
        "ydl = youtube_dl.YoutubeDL(ydl_opts)\n",
        "\n",
        "# Download the videos\n",
        "with ydl:\n",
        "    search_results = ydl.extract_info(\"ytsearch\" + str(num_videos) + \":\" + search_query, download=False)\n",
        "    video_urls = [result['webpage_url'] for result in search_results['entries']]\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Create the thread pool executor\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = []\n",
        "\n",
        "    # Download and convert each video to audio in parallel\n",
        "    for i, video_url in enumerate(video_urls):\n",
        "        video_file = f\"{download_directory}video_{i+1}.mp4\"\n",
        "        audio_file = f\"{output_directory}audio_{i+1}.mp3\"\n",
        "\n",
        "        # Download the video using youtube-dl\n",
        "        executor.submit(ydl.download, [video_url])\n",
        "\n",
        "        # Convert the video to audio using moviepy\n",
        "        future = executor.submit(convert_to_audio, video_file, audio_file)\n",
        "        futures.append(future)\n",
        "\n",
        "    # Wait for all the conversions to complete\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "print(\"Pipeline completed successfully.\")\n"
      ],
      "metadata": {
        "id": "qV2P4YC0SWse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. Create an automated pipeline using multi-threading for: “Automatic Download of 500 images of Dog from \n",
        "GoogleImages” → “Rescale it to 50%”.\n"
      ],
      "metadata": {
        "id": "gL6fY9EvSiYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "from google_images_download import google_images_download\n",
        "from PIL import Image\n",
        "\n",
        "# Define the search query and other download parameters\n",
        "search_query = \"dog\"\n",
        "num_images = 500\n",
        "download_directory = \"./dog_images/\"\n",
        "output_directory = \"./rescaled_dog_images/\"\n",
        "rescale_percentage = 50\n",
        "\n",
        "# Set the options for image download\n",
        "arguments = {\n",
        "    \"keywords\": search_query,\n",
        "    \"limit\": num_images,\n",
        "    \"output_directory\": download_directory,\n",
        "    \"no_directory\": True,\n",
        "    \"chromedriver\": \"path_to_chromedriver\"  # Path to chromedriver executable (required for download)\n",
        "}\n",
        "\n",
        "# Create an instance of the GoogleImagesDownload class\n",
        "response = google_images_download.googleimagesdownload()\n",
        "\n",
        "# Download the images\n",
        "response.download(arguments)\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Function to rescale an image\n",
        "def rescale_image(image_path, output_path, percentage):\n",
        "    image = Image.open(image_path)\n",
        "    width = int(image.width * (percentage / 100))\n",
        "    height = int(image.height * (percentage / 100))\n",
        "    resized_image = image.resize((width, height))\n",
        "    resized_image.save(output_path)\n",
        "\n",
        "# Get the downloaded image paths\n",
        "image_paths = [os.path.join(download_directory, file_name) for file_name in os.listdir(download_directory)]\n",
        "\n",
        "# Create the thread pool executor\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = []\n",
        "\n",
        "    # Rescale each image in parallel\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        output_path = os.path.join(output_directory, f\"rescaled_dog_{i+1}.jpg\")\n",
        "        future = executor.submit(rescale_image, image_path, output_path, rescale_percentage)\n",
        "        futures.append(future)\n",
        "\n",
        "    # Wait for all the rescaling tasks to complete\n",
        "    concurrent.futures.wait(futures)\n",
        "\n",
        "print(\"Pipeline completed successfully.\")\n"
      ],
      "metadata": {
        "id": "bFgQUPXISpJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part II: Data Analytics**"
      ],
      "metadata": {
        "id": "2uDq-gr0SwFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. Create a random dataset of 100 rows and 30 columns. All the values are defined between [1,200]. "
      ],
      "metadata": {
        "id": "iuRvbig0S6Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the size of the dataset\n",
        "num_rows = 100\n",
        "num_columns = 30\n",
        "\n",
        "# Generate the random dataset\n",
        "dataset = np.random.randint(low=1, high=201, size=(num_rows, num_columns))\n",
        "\n",
        "# Print the dataset\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "xAq8ZjLeS-5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform \n",
        "the following operations:\n",
        "\n",
        "(i) Replace all the values with NA in the dataset defined between [10, 60]. Print the count of number \n",
        "rows having missing values.\n",
        "(ii) Replace all the NA values with the average of the column value. \n",
        "(iii) Find the Pearson correlation among all the columns and plot heat map. Also select those columns \n",
        "having correlation <=0.7.\n",
        "(iv) Normalize all the values in the dataset between 0 and 10.\n",
        "(v) Replace all the values in the dataset with 1 if value <=0.5 else with 0."
      ],
      "metadata": {
        "id": "AMhvvNgNTfeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step (i): Replace values with NA in the range [10, 60]\n",
        "dataset[(dataset >= 10) & (dataset <= 60)] = np.nan\n",
        "\n",
        "# Count the number of rows with missing values\n",
        "rows_with_missing_values = dataset.isnull().sum(axis=1)\n",
        "num_rows_with_missing_values = len(rows_with_missing_values[rows_with_missing_values > 0])\n",
        "print(\"Number of rows with missing values:\", num_rows_with_missing_values)\n",
        "\n",
        "# Step (ii): Replace NA values with column averages\n",
        "dataset = dataset.fillna(dataset.mean())\n",
        "\n",
        "# Step (iii): Calculate Pearson correlation and plot heatmap\n",
        "correlation_matrix = dataset.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlBu\")\n",
        "plt.title(\"Pearson Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Select columns with correlation <= 0.7\n",
        "columns_with_low_correlation = correlation_matrix.columns[correlation_matrix.max() <= 0.7]\n",
        "print(\"Columns with correlation <= 0.7:\", columns_with_low_correlation)\n",
        "\n",
        "# Step (iv): Normalize values between 0 and 10\n",
        "normalized_dataset = (dataset - dataset.min()) / (dataset.max() - dataset.min()) * 10\n",
        "\n",
        "# Step (v): Replace values <= 0.5 with 1, else with 0\n",
        "binary_dataset = np.where(normalized_dataset <= 0.5, 1, 0)\n",
        "\n",
        "# Print the final binary dataset\n",
        "print(\"Binary dataset:\")\n",
        "print(binary_dataset)\n"
      ],
      "metadata": {
        "id": "RYSzHDwWT4Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. Create a random dataset of 500 rows and 10 columns. \n",
        "Columns 1 to 4 are defined between [-10, 10]; \n",
        "Columns 5 to 8 are defined between [10, 20]; \n",
        "Columns 9 to 10 are defined between [-100, 100]. "
      ],
      "metadata": {
        "id": "sE_brUANUQS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the size of the dataset\n",
        "num_rows = 500\n",
        "num_columns = 10\n",
        "\n",
        "# Define the ranges for each column\n",
        "column_ranges = [\n",
        "    (-10, 10),   # Columns 1 to 4\n",
        "    (10, 20),    # Columns 5 to 8\n",
        "    (-100, 100)  # Columns 9 to 10\n",
        "]\n",
        "\n",
        "# Generate the random dataset\n",
        "dataset = np.zeros((num_rows, num_columns))\n",
        "\n",
        "for i, (low, high) in enumerate(column_ranges):\n",
        "    dataset[:, i] = np.random.uniform(low=low, high=high, size=num_rows)\n",
        "\n",
        "# Convert the dataset to a DataFrame for easier manipulation\n",
        "df = pd.DataFrame(dataset, columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Print the dataset\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "y-g3OkTyUYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply following clustering algorithms; determine the optimal number of clusters and plot distance metric \n",
        "graph using each algorithm.\n",
        "(i) K-Mean clustering\n",
        "(ii) Hierarchical clustering"
      ],
      "metadata": {
        "id": "yBAarNTcUdHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#K Means\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Load the dataset or use your existing dataset\n",
        "# dataset = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Generate a random dataset for demonstration purposes\n",
        "np.random.seed(0)\n",
        "num_rows = 500\n",
        "num_columns = 10\n",
        "dataset = pd.DataFrame(np.random.rand(num_rows, num_columns), columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Convert the dataset to a NumPy array\n",
        "X = dataset.values\n",
        "\n",
        "# Determine the optimal number of clusters for K-Means using the elbow method\n",
        "distortions = []\n",
        "max_clusters = 10\n",
        "for n_clusters in range(1, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    distortions.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the distance metric graph (elbow curve) for K-Means\n",
        "plt.plot(range(1, max_clusters + 1), distortions, marker='o')\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Distortion\")\n",
        "plt.title(\"K-Means Clustering - Elbow Curve\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TzN9OhUIUpwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hierarchical \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Load the dataset or use your existing dataset\n",
        "# dataset = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Generate a random dataset for demonstration purposes\n",
        "np.random.seed(0)\n",
        "num_rows = 500\n",
        "num_columns = 10\n",
        "dataset = pd.DataFrame(np.random.rand(num_rows, num_columns), columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Convert the dataset to a NumPy array\n",
        "X = dataset.values\n",
        "\n",
        "# Determine the optimal number of clusters for Hierarchical clustering using the dendrogram\n",
        "distances = pairwise_distances(X)\n",
        "linkage = AgglomerativeClustering(n_clusters=None, distance_threshold=0)\n",
        "linkage.fit(distances)\n",
        "\n",
        "# Plot the distance metric graph (dendrogram) for Hierarchical clustering\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Hierarchical Clustering - Dendrogram\")\n",
        "plt.xlabel(\"Data Point Index\")\n",
        "plt.ylabel(\"Distance\")\n",
        "dendrogram = linkage.linkage_\n",
        "plt.plot(range(1, len(dendrogram) + 1), dendrogram)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gpc1vM9SU0k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. Create a random dataset of 600 rows and 15 columns. All the values are defined between [-100,100]. \n"
      ],
      "metadata": {
        "id": "vLuR2Q4sVDL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the size of the dataset\n",
        "num_rows = 600\n",
        "num_columns = 15\n",
        "\n",
        "# Define the range for the values\n",
        "value_range = (-100, 100)\n",
        "\n",
        "# Generate the random dataset\n",
        "dataset = np.random.randint(low=value_range[0], high=value_range[1]+1, size=(num_rows, num_columns))\n",
        "\n",
        "# Convert the dataset to a DataFrame for easier manipulation\n",
        "df = pd.DataFrame(dataset, columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Print the dataset\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "W4EqVxjPVNSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the following operations:\n",
        "(i) Plot scatter graph between Column 5 and Column 6.\n",
        "(ii) Plot histogram of each column in single graph.\n",
        "(iii) Plot the Box plot of each column in single graph.\n"
      ],
      "metadata": {
        "id": "cZANJ5o8VTu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset or use your existing dataset\n",
        "# df = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Generate a random dataset for demonstration purposes\n",
        "np.random.seed(0)\n",
        "num_rows = 600\n",
        "num_columns = 15\n",
        "dataset = pd.DataFrame(np.random.randint(low=-100, high=101, size=(num_rows, num_columns)),\n",
        "                       columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Step (i): Plot scatter graph between Column 5 and Column 6\n",
        "plt.scatter(dataset['Column 5'], dataset['Column 6'])\n",
        "plt.xlabel('Column 5')\n",
        "plt.ylabel('Column 6')\n",
        "plt.title('Scatter Plot: Column 5 vs Column 6')\n",
        "plt.show()\n",
        "\n",
        "# Step (ii): Plot histogram of each column in a single graph\n",
        "dataset.hist(figsize=(12, 8))\n",
        "plt.suptitle('Histograms of Columns', y=0.95)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step (iii): Plot the Box plot of each column in a single graph\n",
        "dataset.plot.box(figsize=(12, 8))\n",
        "plt.title('Box Plot of Columns')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Values')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uz-iZ9QPVe4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. Create a random dataset of 500 rows and 5 columns:\n",
        "All the values are defined between [5,10]."
      ],
      "metadata": {
        "id": "s9c79xg_ViMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set the size of the dataset\n",
        "num_rows = 500\n",
        "num_columns = 5\n",
        "\n",
        "# Define the range for the values\n",
        "value_range = (5, 10)\n",
        "\n",
        "# Generate the random dataset\n",
        "dataset = np.random.uniform(low=value_range[0], high=value_range[1], size=(num_rows, num_columns))\n",
        "\n",
        "# Convert the dataset to a DataFrame for easier manipulation\n",
        "df = pd.DataFrame(dataset, columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Print the dataset\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "-VDGNpTIVnb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the following operations:\n",
        "(i) Perform t-Test on each column.\n",
        "(ii) Perform Wilcoxon Signed Rank Test on each column.\n",
        "(iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4"
      ],
      "metadata": {
        "id": "qnq48mDaVrX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Load the dataset or use your existing dataset\n",
        "# df = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Generate a random dataset for demonstration purposes\n",
        "np.random.seed(0)\n",
        "num_rows = 500\n",
        "num_columns = 5\n",
        "dataset = pd.DataFrame(np.random.uniform(low=5, high=10, size=(num_rows, num_columns)),\n",
        "                       columns=[f\"Column {i+1}\" for i in range(num_columns)])\n",
        "\n",
        "# Step (i): Perform t-Test on each column\n",
        "t_test_results = {}\n",
        "for column in dataset.columns:\n",
        "    t_statistic, p_value = stats.ttest_1samp(dataset[column], 7.5)  # Assuming null hypothesis mean = 7.5\n",
        "    t_test_results[column] = {\"t_statistic\": t_statistic, \"p_value\": p_value}\n",
        "\n",
        "# Step (ii): Perform Wilcoxon Signed Rank Test on each column\n",
        "wilcoxon_results = {}\n",
        "for column in dataset.columns:\n",
        "    statistic, p_value = stats.wilcoxon(dataset[column] - 7.5)  # Assuming null hypothesis median = 7.5\n",
        "    wilcoxon_results[column] = {\"statistic\": statistic, \"p_value\": p_value}\n",
        "\n",
        "# Step (iii): Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4\n",
        "column3 = dataset['Column 3']\n",
        "column4 = dataset['Column 4']\n",
        "t_test_2samp_result = stats.ttest_ind(column3, column4)\n",
        "wilcoxon_ranksum_result = stats.ranksums(column3, column4)\n",
        "\n",
        "# Print the results\n",
        "print(\"t-Test Results:\")\n",
        "for column, result in t_test_results.items():\n",
        "    print(f\"Column {column}: t-statistic = {result['t_statistic']:.4f}, p-value = {result['p_value']:.4f}\")\n",
        "\n",
        "print(\"\\nWilcoxon Signed Rank Test Results:\")\n",
        "for column, result in wilcoxon_results.items():\n",
        "    print(f\"Column {column}: statistic = {result['statistic']:.4f}, p-value = {result['p_value']:.4f}\")\n",
        "\n",
        "print(\"\\nTwo Sample t-Test Result (Column 3 vs Column 4):\")\n",
        "print(f\"t-statistic = {t_test_2samp_result.statistic:.4f}, p-value = {t_test_2samp_result.pvalue:.4f}\")\n",
        "\n",
        "print(\"\\nWilcoxon Rank Sum Test Result (Column 3 vs Column 4):\")\n",
        "print(f\"statistic = {wilcoxon_ranksum_result.statistic:.4f}, p-value = {wilcoxon_ranksum_result.pvalue:.4f}\")\n"
      ],
      "metadata": {
        "id": "pia5qWjTWNIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}